# -*- coding: utf-8 -*-
"""ModelGeneration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GIhrQ0OIamIrFnCv6ESAcM1_dPnscdWF
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

import sys
# Change the path as per your system
sys.path.append("/home/sumedha/Documents/Subjects/Mathematics/Project")
df = pd.read_csv("Breast_Cancer.csv")

categorical_features = df.select_dtypes(include = "object").columns
numerical_features = df.select_dtypes(include = "int64").columns

from sklearn.preprocessing import LabelEncoder

# Apllying LabelEncoder to all categorical features
df[categorical_features] = df[categorical_features].apply(LabelEncoder().fit_transform)

# Displaying first 5 rows
#df.head()

from sklearn.preprocessing import RobustScaler, StandardScaler

# Saving tumor size for later
tumor_sizes = df["Tumor Size"].to_numpy()

# Applying RobustScaler to scale orginal numerical features
df[numerical_features] = RobustScaler().fit_transform(df[numerical_features])

# Displaying first 5 rows
df.describe()

df

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Assuming you have a dataframe 'df' with shape (4024, 16) containing your dataset

# Step 1: Select the response variable (Y)
response_variable = 'Tumor Size'  # Replace with the name of your response variable column
Y = df[response_variable]

# Step 2: Remove the response variable from the dataset
df_filtered = df.drop(columns=[response_variable])


# Step 3: Remove highly correlated variables
correlation_threshold = 0.60
corr_matrix = np.array(df_filtered.corr())  # Convert correlation matrix to NumPy array
upper_tri = np.triu(corr_matrix, k=1)
highly_correlated_vars = [column for column in df_filtered.columns if any(upper_tri[df_filtered.columns.get_loc(column)] > correlation_threshold)]

# Step 4: Compute the eigenvalue decomposition
correlation_matrix = np.corrcoef(df_filtered.T)
eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)

# Step 5: Remove the eigenvectors corresponding to highly correlated variables
eigenvectors_filtered = eigenvectors[:, ~np.isin(range(len(df_filtered.columns)), highly_correlated_vars)]

# Step 6: Matrix transformation
X_transformed = df_filtered.dot(eigenvectors_filtered)

# Step 7: Data division into train and test datasets
X_train, X_test, Y_train, Y_test = train_test_split(X_transformed, Y, test_size=0.2, random_state=42)

# Step 8: Train the SVR model
regressor = SVR(kernel='linear')
regressor.fit(X_train, Y_train)

### using ANN to train model
ann = MLPRegressor(hidden_layer_sizes=(10,), activation='relu', solver='adam', max_iter=200)
ann.fit(X_train, Y_train)

# Step 9: Predict on test data
Y_pred = regressor.predict(X_test)
Y_pred_ann = ann.predict(X_test)

# Step 10: Evaluate the model
mse = mean_squared_error(Y_test, Y_pred)
mse_ann = mean_squared_error(Y_test, Y_pred_ann)

print("Mean Squared Error for SVR is :", mse)
print("Mean Squared Error for ANN is :", mse_ann)

# Calculate R-squared
r2 = r2_score(Y_test, Y_pred)
r2_ann = r2_score(Y_test, Y_pred_ann)

print("R-squared for SVR is:", r2)
print("R-squared for ANN is:", r2_ann)

mae = mean_absolute_error(Y_test, Y_pred)
mae_ann = mean_absolute_error(Y_test, Y_pred_ann)
print("Mean Absolute Error for SVR is:", mae)
print("Mean Absolute Error for ANN is:", mae_ann)

highly_correlated_vars

import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.1, 1.0, 10.0]
}

# Create the SVR model
svr = SVR()

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5)
grid_search.fit(X_train, Y_train)

# Get the best SVR model and its corresponding hyperparameters
best_svr = grid_search.best_estimator_
best_params = grid_search.best_params_

# Plot the parameter search results
param_names = list(param_grid.keys())
param_values = [param_grid[param] for param in param_names]
mean_scores = grid_search.cv_results_['mean_test_score']


# Make predictions on the test set
y_pred = best_svr.predict(X_test)

# Evaluate the performance
R2_score_svr = r2_score(Y_test, y_pred)
mse_svr = mean_squared_error(Y_test, y_pred)
print("R2 score:", R2_score_svr)
print("MSE:", mse_svr)
print("Best Hyperparameters:", best_params)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'hidden_layer_sizes': [(10,), (50,), (100,)]
}

# Create the MLPRegressor model
mlp = MLPRegressor(random_state=42, max_iter=500, activation='tanh')

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=5)
grid_search.fit(X_train, Y_train)

# Get the best MLPRegressor model and its corresponding hyperparameters
best_mlp = grid_search.best_estimator_
best_params = grid_search.best_params_

# Plot the parameter search results
param_names = list(param_grid.keys())
param_values = [param_grid[param] for param in param_names]
mean_scores = grid_search.cv_results_['mean_test_score']


# Reshape param_values and mean_scores arrays
param_values = np.array(param_values).reshape(-1, 1)
mean_scores = np.array(mean_scores).reshape(-1, 1)


# Make predictions on the test set
y_pred_ann_cv = best_mlp.predict(X_test)

# Evaluate the performance
R2_score = r2_score(Y_test, y_pred_ann_cv)
mse = mean_squared_error(Y_test, y_pred_ann_cv)
print("R2score:", R2_score)
print("MSE:", mse)
print("Best Hyperparameters:", best_params)



import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'C': [0.1, 1.0, 10.0]
}

# Create the SVR model
svr = SVR()

# Perform grid search with cross-validation
grid_search = GridSearchCV(estimator=svr, param_grid=param_grid, cv=5)
grid_search.fit(X_train, Y_train)

# Get the best SVR model and its corresponding hyperparameters
best_svr = grid_search.best_estimator_
best_params = grid_search.best_params_

# Plot the parameter search results
param_names = list(param_grid.keys())
param_values = [param_grid[param] for param in param_names]
mean_scores = grid_search.cv_results_['mean_test_score']


# Make predictions on the test set
y_pred = best_svr.predict(X_test)

# Evaluate the performance
R2_score_svr = r2_score(Y_test, y_pred)
mse_svr = mean_squared_error(Y_test, y_pred)
print("R2 score:", R2_score_svr)
print("MSE:", mse_svr)
print("Best Hyperparameters:", best_params)

